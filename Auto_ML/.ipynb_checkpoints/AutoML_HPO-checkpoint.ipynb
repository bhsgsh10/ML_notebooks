{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "\n",
    "## Hyperparameter Optimization (HPO)\n",
    "Automated HPO has the following advantages:\n",
    "- reduce the human effort necessary for applying machine learning. This is particularly important in the context of AutoML.\n",
    "- improve the performance of machine learning algorithms (by tailoring them to the problem at hand); this has led to new state-of-the-art performances for important machine learning benchmarks in several studies \n",
    "- improve the reproducibility and fairness of scientific studies. Automated HPO is clearly more reproducible than manual search. It facilitates fair comparisons since different methods can only be compared fairly if they all receive the same level of tuning for the problem at hand \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO faces several challenges which make it a hard problem in practice:\n",
    "- Function evaluations can be extremely expensive for large models (e.g., in deep learning), complex machine learning pipelines, or large datesets.   \n",
    "- The configuration space is often complex (comprising a mix of continuous, categorical and conditional hyperparameters) and high-dimensional. Furthermore, it is not always clear which of an algorithm’s hyperparameters need to be optimized, and in which ranges.   \n",
    "- We usually don’t have access to a gradient of the loss function with respect to the hyperparameters. Furthermore, other properties of the target function often used in classical optimization do not typically apply, such as convexity and smoothness.   \n",
    "- One cannot directly optimize for generalization performance as training datasets are of limited size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A$ denote a machine learning algorithm with $N$ hyperparameters. Domain of $n^{th}$ hyperparameter is denoted as $K_n$. A vector of hyperparameters is given by $\\lambda$ and $A$ instantiated with $\\lambda$ is denoted by $A_\\lambda$.   \n",
    "Then the optimum state of $\\lambda$ is given by:\n",
    "\n",
    "$$ λ^{*} = argmin_{λ∈K} E_{(D_{train},D_{valid} )∼D} V(L, A_λ, D_{train}, D_{valid} ) $$\n",
    "where $V(L, A_λ, D_{train}, D_{valid}$ measures the loss of a model generated by algorithm $A$ with hyperparameters $\\lambda$ on training data $D_{train}$ and evaluated on validation data $D_{valid}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blackbox HPO\n",
    "#### Model free Blackbox HPO\n",
    "\n",
    "**Grid search:**   \n",
    "- Required number of function evaluations grows exponentially with the dimensionality of the configuration space of hyperparameters. \n",
    "- Increasing the resolution of discretization substantially increases the required number of function evaluations.\n",
    "\n",
    "**Randomized search** works better than Grid search because it searches randomly over configurations until a certain budget or threshold for the search is exhausted. Also in cases where one hyperparameter is more important than the other, randomized search performs better.   \n",
    "Further advantages over grid search include **easier parallelization** (since workers\n",
    "do not need to communicate with each other and failing workers do not leave holes\n",
    "in the design) and **flexible resource allocation** (since one can add an arbitrary number\n",
    "of random points to a random search design to still yield a random search design;\n",
    "the equivalent does not hold for grid search).\n",
    "\n",
    "**Use Randomized search for high dimensional configuration space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization\n",
    "Bayesian optimization is an iterative algorithm with two key ingredients: a probabilitistic surrogate model and an acqisition function to decide which point to evaluate next.   \n",
    "**In each iteration, the surrogate model is fitted to all observations\n",
    "of the target function made so far. Then the acquisition function, which uses the\n",
    "predictive distribution of the probabilistic model, determines the utility of different\n",
    "candidate points, trading off exploration and exploitation. Compared to evaluating\n",
    "the expensive blackbox function, the acquisition function is cheap to compute and\n",
    "can therefore be thoroughly optimized.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bayesian_optimization.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We care about the optimum, so we look at how the graph/curve looks around that area of the acquistion function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multifidelity optimization\n",
    "Cheap surrogate running on part of the dataset, that can be used to retrieve some information about the quality of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Multifidelity_search.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure illustrates that even when we take 10% of the data (RBF-SVM) or 10 trees (random forest) and run a grid search over `gamma` and `C` or `max_features` and `max_depth`, we can still rule out a bunch of hyperparameter values.   \n",
    "For RBF-SVM, looking at the first figure itself tells us that optimum values for `gamma` and `C` don't lie in the upper left or the bottom right corner. Similarly, for random forest, optimum value for `max_depth` does not lie below 6 and that for `max_features` is probably below 64. We can make these conclusions without using a larger subset of the data or using larger no. of estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Successive halving\n",
    "Combining grid search or randomized search with multi fidelity search\n",
    "- Given n configuration and budget B\n",
    "- pick η=2 or η=3 (wording follows 2)\n",
    "- Each iteration, keep best halve of configurations after $k=log_η(n)+1$ left with single configuration.\n",
    "- initially allocate B/kn to each configuration, double each iteration\n",
    "\n",
    "Parameter space is reduced by 1/η after each iteration. The number of estimators will grow η times.\n",
    "\n",
    "Drawback of this approach is that some configurations, that perform poorly with limited budget (estimators/data) and are discarded early on in the process, may actually perform better if allowed more budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperband\n",
    "Improvement over successive halving. Performs various versions of halving.   \n",
    "It divides the total budget into\n",
    "several combinations of number of configurations vs. budget for each, to then call\n",
    "successive halving as a subroutine on each set of random configurations. Due to the\n",
    "hedging strategy which includes running some configurations only on the maximal\n",
    "budget, in the worst case, HyperBand takes at most a constant factor more time\n",
    "than vanilla random search on the maximal budget. In practice, due to its use\n",
    "of cheap low-fidelity evaluations, HyperBand has been shown to improve over\n",
    "vanilla random search and blackbox Bayesian optimization for data subsets, feature\n",
    "subsets and iterative algorithms, such as stochastic gradient descent for deep neural\n",
    "networks.\n",
    "\n",
    "Bayesian Optimization plus Hyperband can provide upto 50 times better timing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "86066cc6-8cc5-49f7-992f-3316eee68482"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
